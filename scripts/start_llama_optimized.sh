#!/bin/bash
# Optimiertes Start-Skript f√ºr llama.cpp mit Streaming-Unterst√ºtzung

MODEL_PATH="./models/Qwen3-4B-Q5_K_M.gguf"
BACKUP_MODEL="./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf"

echo "üöÄ Starte llama-server mit Streaming-Optimierungen..."
echo "Model: $(basename $MODEL_PATH)"
echo "Port: 5020"
echo ""

# Pr√ºfe ob Modell existiert
if [ ! -f "$MODEL_PATH" ]; then
    echo "‚ö†Ô∏è  Hauptmodell nicht gefunden: $MODEL_PATH"
    if [ -f "$BACKUP_MODEL" ]; then
        echo "‚úÖ Verwende Backup-Modell: $(basename $BACKUP_MODEL)"
        MODEL_PATH="$BACKUP_MODEL"
    else
        echo "‚ùå Kein Modell gefunden! Bitte Modelle herunterladen."
        exit 1
    fi
fi

# CPU-Kerne ermitteln f√ºr optimale Thread-Konfiguration
CPU_CORES=$(sysctl -n hw.ncpu)
THREADS=$((CPU_CORES - 2))  # 2 Kerne f√ºr System reservieren
if [ $THREADS -lt 4 ]; then
    THREADS=4
fi

echo "üîß System-Informationen:"
echo "   CPU-Kerne: $CPU_CORES"
echo "   Threads f√ºr LLM: $THREADS"
echo "   Parallel Slots: 4"
echo "   HTTP-Threads: 4"
echo ""

# Starte llama-server mit Streaming-Optimierungen
./llama.cpp/build/bin/llama-server \
  -m "$MODEL_PATH" \
  --host 0.0.0.0 \
  --port 5020 \
  --ctx-size 2048 \
  --threads $THREADS \
  --gpu-layers 0 \
  --cont-batching \
  --parallel 4 \
  --threads-http 4 \
  --timeout 300 \
  --log-format json \
  --log-disable \
  --metrics

echo ""
echo "‚úÖ llama-server gestartet mit folgenden Features:"
echo "   ‚Ä¢ Streaming √ºber Server-Sent Events (SSE)"
echo "   ‚Ä¢ Kontinuierliches Batching (--cont-batching)"
echo "   ‚Ä¢ 4 parallele Anfragen (--parallel 4)"
echo "   ‚Ä¢ 5 Minuten Timeout f√ºr lange Sessions"
echo "   ‚Ä¢ JSON-Logging f√ºr Monitoring"
echo ""
echo "üì° API-Endpunkte:"
echo "   ‚Ä¢ Chat Completions: http://localhost:5020/v1/chat/completions"
echo "   ‚Ä¢ Completions:      http://localhost:5020/v1/completions"
echo "   ‚Ä¢ Health Check:     http://localhost:5020/health"
echo ""
echo "üîç Streaming aktivieren mit: \"stream\": true in API-Requests"
echo "üìä Monitoring: tail -f logs/llama_server.log"
